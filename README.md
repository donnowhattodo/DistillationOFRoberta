# DistillationOFRoberta
# RoBERTa Knowledge Distillation

This repository demonstrates knowledge distillation from a RoBERTa teacher model to a RoBERTa Small student model using Hugging Face Transformers. Using the SST-2 dataset for sentiment classification, we aim to transfer the teacher's knowledge to the student, achieving efficient and accurate sentiment analysis. The process involves loading the dataset, tokenizing, defining models, setting hyperparameters, and a training loop.


